{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 4 : Anticipez les besoins en consommation électrique de bâtiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse exploratoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2015 = pd.read_csv(\"data/2015-building-energy-benchmarking.csv\")\n",
    "data_2016 = pd.read_csv(\"data/2016-building-energy-benchmarking.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploration initiale des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(dataframe):\n",
    "    \"\"\"Calcuates and displays the shape of the dataframe and the filling rage\"\"\"\n",
    "    \n",
    "    # get the numbers of rows and columns in the dataframe\n",
    "    nb_rows, nb_columns = dataframe.shape\n",
    "    print(\"DataFrame has {} rows and {} columns.\".format(nb_rows,nb_columns))\n",
    "\n",
    "    # get the number of non-Nan data in the dataframe\n",
    "    nb_data = dataframe.describe(include='all').loc[\"count\"].sum()\n",
    "\n",
    "    # computing the filling rate by rounding down to 2 decimal places\n",
    "    import math\n",
    "    filling_rate = math.floor(nb_data*10000 / (nb_rows * nb_columns))/10000\n",
    "    print(\"The global filling rate of the DataFrame is : {:.2%}\".format(filling_rate))\n",
    "    \n",
    "    # computing the missing values\n",
    "    nb_missing = int((nb_rows * nb_columns) - nb_data)\n",
    "    print(\"There are {} missing values out of {}.\".format(nb_missing, int(nb_rows * nb_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape(data_2015)\n",
    "shape(data_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que la plupart des lignes (correspondant à un batîment particulier) se retrouvent à la fois dans les données de 2015 et dans celles de 2016.\n",
    "\n",
    "L'agrégation des données de 2015 et de 2016 nécessitera donc de gérer les données dupliquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNames of the {} common columns :\\n\".format(len(set(data_2015)&set(data_2016))), set(data_2015)&set(data_2016))\n",
    "print(\"\\nNames of the {} columns only in 2015 dataset:\\n\".format(len(set(data_2015)-set(data_2016))), set(data_2015)-set(data_2016))\n",
    "print(\"\\nNames of the {} columns only in 2016 dataset:\\n\".format(len(set(data_2016)-set(data_2015))), set(data_2016)-set(data_2015))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que certains colonnes spécifiques ont en fait la même sémantique. Par exemple :\n",
    "* la colonne 'Comment' de 2015 correspond à la colonne 'Comments' de 2016\n",
    "* la colonne 'Zip Codes' de 2015 correspond à la colonne 'ZipCode' de 2016\n",
    "* la colonne 'GHGEmissionsIntensity(kgCO2e/ft2)' de 2015 correspond à la colonne 'GHGEmissionsIntensity' de 2016\n",
    "* la colonne 'GHGEmissions(MetricTonsCO2e)' de 2015 correspond à la colonne 'TotalGHGEmissions' de 2016\n",
    "\n",
    "Nous constatons aussi que certains colonnes spécifiques ont été reformatées. Ainsi, la colonne 'Location' a été séparée en plusieurs colonnes en 2016 :\n",
    "* 'State'\n",
    "* 'City'\n",
    "* 'Address'\n",
    "* 'Longitude'\n",
    "* 'Latitude'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Nettoyage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des données non-explicatives\n",
    "Nous supprimons les variables (colonnes) qui n'apportent aucun élément explicatif :\n",
    "\n",
    "* Dans les deux tables :\n",
    "    * 'DataYear' : année de collecte des données\n",
    "    * 'PropertyName' : Official or common property name.\n",
    "    * 'TaxParcelIdentificationNumber' : Property King County PIN\n",
    "    * 'CouncilDistrictCode' : Property City of Seattle council district.\n",
    "    * 'DefaultData'\n",
    "    * 'ComplianceStatus' : ???\n",
    "\n",
    "\n",
    "* 2015 seulement :\n",
    "    * 'Comment'\n",
    "    * '2010 Census Tracts' : ???\n",
    "    * 'Seattle Police Department Micro Community Policing Plan Areas' :  ???\n",
    "    * 'City Council Districts' : ???\n",
    "    * 'SPD Beats' : ???\n",
    "    * 'Zip Codes'\n",
    "    * 'Location'\n",
    "\n",
    "\n",
    "* 2016 seulement :\n",
    "    * 'ZipCode'\n",
    "    * 'Longitude'\n",
    "    * 'State'\n",
    "    * 'Latitude'\n",
    "    * 'Comments'\n",
    "    * 'Address'\n",
    "    * 'City'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = [         \n",
    "    'DataYear',\n",
    "    'PropertyName',\n",
    "    'TaxParcelIdentificationNumber',\n",
    "    'CouncilDistrictCode',\n",
    "    'DefaultData',\n",
    "    'Comment',\n",
    "    'ComplianceStatus',\n",
    "    '2010 Census Tracts',\n",
    "    'Seattle Police Department Micro Community Policing Plan Areas',\n",
    "    'City Council Districts',\n",
    "    'SPD Beats',\n",
    "    'Zip Codes',\n",
    "    'Location',\n",
    "    'ZipCode',\n",
    "    'Longitude',\n",
    "    'State',\n",
    "    'Latitude',\n",
    "    'Comments',\n",
    "    'Address',\n",
    "    'City'\n",
    "]\n",
    "\n",
    "# Drop several columns\n",
    "for feature in features_to_drop:\n",
    "    \n",
    "    # In the 2015 dataframe\n",
    "    try:\n",
    "        data_2015 = data_2015.drop(feature, axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # In the 2016 dataframe\n",
    "    try:\n",
    "        data_2016 = data_2016.drop(feature, axis=1)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des données issues des relevés de consommation annuels\n",
    "Nous supprimons aussi les variables (colonnes) issues des relevés de consommation annuels (hormis ceux qui seront utilisés comme étiquette pour la régression) :\n",
    "\n",
    "* Dans les deux tables :\n",
    "    * 'SiteEUI(kBtu/sf)' : Site Energy Use Intensity (EUI) is a property's Site Energy Use divided by its gross floor area.\n",
    "    * 'SiteEUIWN(kBtu/sf)' : Weather Normalized (WN) Site Energy Use Intensity (EUI) is a property's WN Site Energy divided by its gross floor area (in square feet).\n",
    "    * 'SourceEUI(kBtu/sf)' : Source Energy Use Intensity (EUI) is a property's Source Energy Use divided by its gross floor area.\n",
    "    * 'SourceEUIWN(kBtu/sf)' : Weather Normalized (WN) Source Energy Use Intensity (EUI) is a property's WN Source Energy divided by its gross floor area.\n",
    "    * 'SiteEnergyUseWN(kBtu)'\n",
    "    * 'SteamUse(kBtu)' : The annual amount of district steam consumed by the property on-site\n",
    "    * 'Electricity(kWh)' : The annual amount of electricity consumed by the property on-site, […] measured in kWh.\n",
    "    * 'Electricity(kBtu)' : The annual amount of electricity consumed by the property on-site, […] measured in thousands of British thermal units (kBtu).\n",
    "    * 'NaturalGas(therms)' : The annual amount of utility-supplied natural gas consumed by the property, measured in therms.\n",
    "    * 'NaturalGas(kBtu)' :\n",
    "    * 'OtherFuelUse(kBtu)'\n",
    "    * 'Outlier'\n",
    "\n",
    "\n",
    "* 2015 uniquement :\n",
    "    * 'GHGEmissionsIntensity(kgCO2e/ft2)' : Total Greenhouse Gas Emissions divided by property's gross floor area, measured in kilograms of carbon dioxide equivalent per square foot.\n",
    "\n",
    "\n",
    "* 2016 uniquement : \n",
    "    * 'GHGEmissionsIntensity', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop2 = [         \n",
    "        'SiteEUI(kBtu/sf)',\n",
    "        'SiteEUIWN(kBtu/sf)',\n",
    "        'SourceEUI(kBtu/sf)',\n",
    "        'SourceEUIWN(kBtu/sf)',\n",
    "        'SiteEnergyUseWN(kBtu)',\n",
    "        'SteamUse(kBtu)',\n",
    "        'Electricity(kWh)',\n",
    "        'Electricity(kBtu)',\n",
    "        'NaturalGas(therms)',\n",
    "        'NaturalGas(kBtu)',\n",
    "        'OtherFuelUse(kBtu)',\n",
    "        'Outlier',\n",
    "        'GHGEmissionsIntensity(kgCO2e/ft2)',\n",
    "        'GHGEmissionsIntensity'\n",
    "]\n",
    "\n",
    "# Drop several columns\n",
    "for feature in features_to_drop2:\n",
    "    \n",
    "    # In the 2015 dataframe\n",
    "    try:\n",
    "        data_2015 = data_2015.drop(feature, axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # In the 2016 dataframe\n",
    "    try:\n",
    "        data_2016 = data_2016.drop(feature, axis=1)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming a column\n",
    "data_2015 = data_2015.rename(columns={'GHGEmissions(MetricTonsCO2e)':\"TotalGHGEmissions\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNames of the {} common columns :\\n\".format(len(set(data_2015)&set(data_2016))), set(data_2015)&set(data_2016))\n",
    "print(\"\\nNames of the {} columns only in 2015 dataset:\\n\".format(len(set(data_2015)-set(data_2016))), set(data_2015)-set(data_2016))\n",
    "print(\"\\nNames of the {} columns only in 2016 dataset:\\n\".format(len(set(data_2016)-set(data_2015))), set(data_2016)-set(data_2015))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concaténation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_2015, data_2016])\n",
    "shape(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion des lignes correspondant à des bâtiments dupliqués\n",
    "Nous identifions les bâtiments dupliqués sur la base de l'identifiant 'OSEBuildingID'.\n",
    "\n",
    "Nous fusionnons ensuite les lignes et supprimons les doublons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates_merging(dataframe, keys):\n",
    "    \"\"\"This function handles duplicates rows in the dataframe, based on a subset of columns (features).\"\"\"\n",
    "\n",
    "    # Checking if there are some duplicated rows\n",
    "    duplicates_mask = dataframe.duplicated(subset=keys, keep=False)\n",
    "    if True not in list(duplicates_mask.unique()):\n",
    "        print(\"No duplicates.\")\n",
    "        return dataframe\n",
    "    \n",
    "    print(\"Number of duplicates rows :\", len(dataframe[duplicates_mask]))\n",
    "    \n",
    "    # Filtering the dataframe to keep only duplicated rows\n",
    "    duplicates_mask = dataframe.duplicated(subset=keys, keep=False)\n",
    "    duplicates_df = dataframe[duplicates_mask]\n",
    "\n",
    "    # Group-by subset of columns used for key, sort=False to speed-up\n",
    "    gb = duplicates_df.groupby(keys, sort=False)\n",
    "\n",
    "    # Initializing aggregated dataframe\n",
    "    agg_df = pd.DataFrame()\n",
    "\n",
    "    # Identification of numerical and non-numerical columns\n",
    "    numeric_columns = list(dataframe.select_dtypes(include=[np.number]).columns.values)\n",
    "\n",
    "    # defining aggregation function for non-numerical columns\n",
    "    def agg_mode(x): m = pd.Series.mode(x); return m.values[0] if not m.empty else np.nan\n",
    "\n",
    "    # Iterating upon columns\n",
    "    for column in dataframe.columns:\n",
    "\n",
    "        # Calculate the mean of each group for numeric columns\n",
    "        if column in numeric_columns:\n",
    "            agg_col = gb[column].agg('mean')\n",
    "\n",
    "        # Calculate the mode of each group for numeric columns\n",
    "        else:\n",
    "            agg_col = gb[column].agg(agg_mode)\n",
    "            \n",
    "        # adding the aggregated column to aggregated dataframe\n",
    "        agg_df = pd.concat([agg_df, agg_col], axis=1, sort=True)\n",
    "\n",
    "    # Dropping all duplicates\n",
    "    dataframe_cleaned = dataframe.drop_duplicates(subset=keys, keep=False)\n",
    "\n",
    "    # Concatenating the dataframe without duplicates and the aggregated rows for duplicates\n",
    "    result = dataframe_cleaned.append(agg_df, ignore_index=True, sort=False)\n",
    "\n",
    "    print(\"Shape after handling duplicates :\", result.shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = data\n",
    "keys = ['OSEBuildingID']\n",
    "\n",
    "data = duplicates_merging(dataframe, keys)\n",
    "shape(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous éliminons l'identifiant des bâtiments qui ne nous sera plus utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the identifier of buildings\n",
    "data = data.drop('OSEBuildingID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Features engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étude du taux de remplissage des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rows, nb_columns = data.shape\n",
    "\n",
    "# Count of the values on each column\n",
    "columns_count = data.count()\n",
    "\n",
    "# Sort the Series\n",
    "columns_count = columns_count.sort_values(ascending=False)\n",
    "\n",
    "# Calculating filling rates\n",
    "filling_rates = columns_count / nb_rows\n",
    "\n",
    "# Display the filling rates\n",
    "filling_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Élimination des colonnes trop peu remplies\n",
    "Nous décidons de ne conserver que les colonnes remplies à 75% minimum pour la modélisation.\n",
    "\n",
    "Cela nous permet de conserver la variable ENERGYSTARScore dont nous devrons étudier la pertinence (dans la partie modélisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillingrate_filter_columns(dataframe, limit_rate):\n",
    "    \"\"\"This function drop the colums where the filling rate is less than a defined limit rate.\"\"\"\n",
    "\n",
    "    # Count of the values on each column\n",
    "    columns_count = dataframe.count()\n",
    "\n",
    "    # Number of rows in the dataframe\n",
    "    nb_rows = dataframe.shape[0]\n",
    "    \n",
    "    # Calculating filling rates\n",
    "    filling_rates = columns_count / nb_rows\n",
    "\n",
    "    # Define a mask of features with a filling_rate bigger than the limit rate\n",
    "    mask = filling_rates > limit_rate\n",
    "    \n",
    "    # Apply the mask to the filling_rates Series\n",
    "    filling_rates_selection = filling_rates[mask]\n",
    "    \n",
    "    # Get the list of the name of the selected columns\n",
    "    features_selection = list(filling_rates_selection.index)\n",
    "    print(\"Number of columns with a filling rate bigger than  {:.2%} : {} columns.\".format(limit_rate, len(features_selection)))\n",
    "\n",
    "    # Return a projection on the selection of features\n",
    "    return dataframe[features_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fillingrate_filter_columns(data, 0.75)\n",
    "shape(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étude du remplissage des lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the non-null values on each row\n",
    "row_count = data.count(axis=1)\n",
    "\n",
    "# Calculating filling rates\n",
    "nb_columns = data.shape[1]\n",
    "filling_rates_row = row_count / nb_columns\n",
    "\n",
    "# Plotting histogram\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plt.title(\"Distribution du remplissage par produit\", fontsize=25)\n",
    "plt.xlabel(\"taux de remplissage\", fontsize=15)\n",
    "plt.ylabel(\"nombre de produits\", fontsize=15)\n",
    "ax.xaxis.set_major_formatter(ticker.PercentFormatter(xmax=1))\n",
    "bins = np.linspace(0, 1, num=11)\n",
    "ax.hist(filling_rates_row, bins=bins)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que la majeur partie des lignes est remplie à plus de 90%.\n",
    "Nous allons faire de l'imputation pour les dernières valeurs manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation des valeurs manquantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at missing values\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation multivariée de la variable 'ENERGYSTARScore'\n",
    "La variable 'ENERGYSTARScore' est celle qui, parmi les variables conservées, possède le plus mauvais taux de remplissage.\n",
    "\n",
    "Nous allons donc faire une imputation multivariée à l'aide de l'imputeur itératif de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of numeric columns\n",
    "numeric_columns = list(data.select_dtypes(include=['number']).columns)\n",
    "\n",
    "# Selection of columns to apply the imputer, avoiding data leaks\n",
    "numeric_columns.remove('SiteEnergyUse(kBtu)')\n",
    "numeric_columns.remove('TotalGHGEmissions')\n",
    "columns_to_impute = numeric_columns\n",
    "\n",
    "try:\n",
    "    # Load libraries\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer    \n",
    "\n",
    "# Instructions if problem with IterativeImputer\n",
    "except:\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    # Create simple imputer\n",
    "    imp = SimpleImputer(strategy='median')\n",
    "\n",
    "# Instructions if no problem with IterativeImputer\n",
    "else:\n",
    "    # Create iterative imputer\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "    \n",
    "# Instruction to run in all cases\n",
    "finally:\n",
    "    \n",
    "    # Train and apply (inplace) the imputer\n",
    "    data[columns_to_impute] = imp.fit_transform(data[columns_to_impute])\n",
    "\n",
    "    # Checking the effects of the iterative imputer\n",
    "    shape(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation des autres valeurs manquantes\n",
    "Nous allons regarder plus en détail à quoi correspondent les dernières valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows with missing data\n",
    "mask = data.isnull().any(axis=1)\n",
    "data[mask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les autres lignes, nous constatons que les valeurs manquantes correspondent aux variables 'LargestPropertyUseType' et 'LargestPropertyUseTypeGFA'.\n",
    "Nous allons imputer :\n",
    "* la variable 'LargestPropertyUseType' avec la variable 'ListOfAllPropertyUseTypes'\n",
    "* la variable 'LargestPropertyUseTypeGFA' avec la variable 'PropertyGFATotal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation of missing values\n",
    "data['LargestPropertyUseType'].fillna(value=data['ListOfAllPropertyUseTypes'], axis=0, inplace=True)\n",
    "data['LargestPropertyUseType'].fillna(value=data['PrimaryPropertyType'], axis=0, inplace=True)\n",
    "data['LargestPropertyUseTypeGFA'].fillna(value=data['PropertyGFATotal'], axis=0, inplace=True)\n",
    "\n",
    "# Dropping a linearly dependant feature\n",
    "data = data.drop('ListOfAllPropertyUseTypes', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows with missing data\n",
    "mask = data.isnull().any(axis=1)\n",
    "data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que les dernières valeurs manquantes correspondent aux étiquettes (valeurs cibles) : total d'énergie consommée et total des émissions de gaz à effet de serre.\n",
    "\n",
    "Nous allons donc conserver ces lignes telles quelles, dans le but d'appliquer à ces batîments le modèle d'apprentissage automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retypage des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of object dtypes\n",
    "obj_columns = list(data.select_dtypes(include='object').columns)\n",
    "print(\"Columns of object dtypes:\\n\", obj_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object dtypes features as categorical data ('category' dtypes)\n",
    "data[obj_columns] = data[obj_columns].astype('category')\n",
    "\n",
    "# Check dtypes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Études univariées des variables cibles**\n",
    "Nous allons étudier la distribution des variables 'SiteEnergyUse(kBtu)' et 'TotalGHGEmissions' et tester leur normalité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicateurs statistiques principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe() pour min-max…\n",
    "data[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions empiriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_distribution(dataframe, feature):\n",
    "    \"\"\"Function plotting the bar plot and a boxplot (as subplots) for a distribution.\"\"\"\n",
    "    \n",
    "    # Loading libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    # filtering non-null data\n",
    "    mask = dataframe[feature].notnull()\n",
    "    data_view = dataframe[mask]\n",
    "    \n",
    "    # Setting the data to plot\n",
    "    x = data_view[feature]\n",
    "    \n",
    "    # Create a figure instance, and the two subplots\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.suptitle(\"Statistical distribution: \" + feature, fontsize=25)\n",
    "    ax1 = fig.add_subplot(211) # histogram\n",
    "    ax2 = fig.add_subplot(212) # boxplot\n",
    "\n",
    "    # Tell distplot to plot on ax1 with the ax argument\n",
    "    sns.distplot(x, ax=ax1)\n",
    "    ax1.set_ylabel(\"Frequency\", fontsize=20)\n",
    "    ax1.set_xlabel(\"\")\n",
    "\n",
    "    # Tell the boxplot to plot on ax2 with the ax argument\n",
    "    medianprops = {'color':\"black\"}\n",
    "    meanprops = {'marker':'o', 'markeredgecolor':'black', 'markerfacecolor':'firebrick'}\n",
    "    sns.boxplot(x,\n",
    "                ax=ax2,\n",
    "                showfliers=True,\n",
    "                medianprops=medianprops,\n",
    "                showmeans=True,\n",
    "                meanprops=meanprops)\n",
    "    ax2.set_xlabel(\"Value\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters before calling the function\n",
    "dataframe = data\n",
    "feature = 'SiteEnergyUse(kBtu)'\n",
    "\n",
    "# Call to the function\n",
    "empirical_distribution(dataframe, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters before calling the function\n",
    "dataframe = data\n",
    "feature = 'TotalGHGEmissions'\n",
    "\n",
    "# Call to the function\n",
    "empirical_distribution(dataframe, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests de normalité (Shapiro-Wilk)\n",
    "\n",
    "Il semble que les variables cibles ('TotalGHGEmissions', 'SiteEnergyUse(kBtu)') ne soient pas normalement distribuées. Nous allons le vérifier avec un test statistique :\n",
    "\n",
    "    H0 : les valeurs de la variable aléatoire pour l'échantillon sont issus d'une population normalement distribuée.\n",
    "\n",
    "    H1 :  les valeurs de la variable aléatoire pour l'échantillon NE sont PAS issus d'une population normalement distribuée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapiro_wilk(dataframe, feature):\n",
    "    '''This function proceed to the Shapiro-Wilk test (for gaussian distribution).\n",
    "    It takes a dataframe and the name of the feature to test.\n",
    "    It filters for non-null vallues of the feature and print the results.'''\n",
    "    \n",
    "    # Loading libraries\n",
    "    from scipy.stats import shapiro\n",
    "\n",
    "    # filtering non-null data for the feature\n",
    "    mask = dataframe[feature].notnull()\n",
    "    data_view = dataframe[mask][feature]\n",
    "\n",
    "    # processing the Shopiro-Wilk test on the filtered data\n",
    "    results = shapiro(data_view)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Shapiro-Wilk test's statistic value is: W = {}\".format(results[0]))\n",
    "    print(\"Shapiro-Wilk test's p-value is: p = {}\".format(results[1]))\n",
    "    print(\"\\nGaussian distribution hypothesis for \\'{}\\' can be rejected at a risk of {}%.\".format(feature, results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = data\n",
    "feature = 'SiteEnergyUse(kBtu)'\n",
    "shapiro_wilk(dataframe, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = data\n",
    "feature = 'TotalGHGEmissions'\n",
    "shapiro_wilk(dataframe, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformée de Box-Cox\n",
    "Nous essayons de transformer les données pour obtenir une distribution gaussienne à l'aide de la [transformée de Box-Cox](https://fr.wikipedia.org/wiki/Transform%C3%A9e_de_Box-Cox) implémentée dans la [librairie scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = data\n",
    "feature = 'TotalGHGEmissions'\n",
    "\n",
    "# Loading library\n",
    "from scipy import stats\n",
    "\n",
    "# Proceeding the Box-Cox transformation\n",
    "feature_transformed, maxlog = stats.boxcox(dataframe[feature])\n",
    "\n",
    "# Printing the results\n",
    "print(\"Value of lambda that maximise log-likelihood function is: {}\".format(maxlog))\n",
    "\n",
    "# Converting the transformed feature (pd.Series) as a pd.DataFrame\n",
    "feature_transformed = pd.DataFrame(feature_transformed, columns = [feature])\n",
    "\n",
    "# Testing the normality of the Box-Cox transformed feature\n",
    "print(\"\\nResults of the Shapiro-Wilk test on transformed feature:\")\n",
    "shapiro_wilk(feature_transformed, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters before calling the function\n",
    "dataframe = feature_transformed\n",
    "feature = 'TotalGHGEmissions'\n",
    "\n",
    "# Call to the function\n",
    "empirical_distribution(dataframe, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Étude des corrélations entre variables**\n",
    "\n",
    "La librairie dython (http://shakedzy.xyz/dython/) permet de calculer :\n",
    "* pour les couples de variables quantitatives :\n",
    "    * le coefficient de correlation de Pearson R \n",
    "    \n",
    "    \n",
    "* pour les couples de variables catégorielles :\n",
    "    * le coefficient de Cramer V (symetrique)\n",
    "    * le coefficient d'incertitude de Theil U (asymetrique)\n",
    "    \n",
    "    \n",
    "* pour les couples de variables mixtes (quantitative + catégorielle) :\n",
    "    * le ratio de correlation η (eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "from dython.nominal import associations\n",
    "\n",
    "# Get the categorical columns\n",
    "categorical_columns = list(data.select_dtypes(include='category').columns)\n",
    "\n",
    "# Have to drop NaN values to avoid errors\n",
    "df_for_correlations = data.dropna()\n",
    "\n",
    "# Calculate associations and display graph\n",
    "associations(\n",
    "    df_for_correlations,\n",
    "    figsize=(15,5),\n",
    "    theil_u=True, # asymetric measure of correlation for nominal feature\n",
    "    nominal_columns=categorical_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons qu'un sous-ensemble des variables sont très fortement corrélées entre elles (**multicolinéarité**).\n",
    "* 'LargestPropertyUseTypeGFA'\n",
    "* 'PropertyGFABuilding(s)'\n",
    "* 'PropertyGFATotal'\n",
    "\n",
    "Pour nous permettre de pouvoir faire une analyse fiable de l'importance des variables (***features importance***) de notre modèle, nous décidons de supprimer ces variables, sauf une."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "data = data.drop(columns=['PropertyGFABuilding(s)', 'PropertyGFATotal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons aussi constater que la variable **'ENERGYSTARScore'** présente un coefficient de corrélation linéaire proche de 0 avec les variables à expliquer ('TotalGHGEmissions', 'SiteEnergyUse(kBtu)').\n",
    "\n",
    "Cela laisse entendre que cette variable ne sera pas très importante pour notre modèle, mais nous le confirmerons avec une analyse de l'importance des variables (*features importance*) a posteriori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualisation des données** (par t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une valeur (nulle) pose problème lors du passage au logarithme, nous allons l'écarter pour la t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification of the values out of the domain of log function\n",
    "mask = data['SiteEnergyUse(kBtu)'] <= 0\n",
    "data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tsne = data.drop(209)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection des variables pour la t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all the numeric columns\n",
    "subset = list(data.select_dtypes(include=['number']).columns)\n",
    "\n",
    "# Keeping only the required columns in the dataframe\n",
    "X = data_tsne[subset]\n",
    "\n",
    "# Dropping rows with missing values (not handled by t-SNE implementation)\n",
    "X = X.dropna(subset=subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-traitement pour la t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering and reducting all numeric columns\n",
    "from sklearn import preprocessing\n",
    "std_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_std = std_scaler.transform(X)\n",
    "\n",
    "# Tranforming with logarithm of the target (only for coloring t-SNE)\n",
    "# y = np.log(X['SiteEnergyUse(kBtu)'])\n",
    "y = X['SiteEnergyUse(kBtu)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exécution de la t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import manifold\n",
    "\n",
    "# Instanciation of t-SNE\n",
    "tsne = manifold.TSNE(n_components=2,\n",
    "                     perplexity=30,\n",
    "                     n_iter=300,\n",
    "                     init='pca', # initialisation by a PCA\n",
    "                     random_state=0\n",
    "                    )\n",
    "\n",
    "# Applying the t-SNE\n",
    "X_projected = tsne.fit_transform(X_std) # t-SNE do not have \".transform\" method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de la t-SNE en 2D\n",
    "Nous procédons à une visualisation des données, en fonction de leur note Nutri-Score ('nutrition_grade_fr'), par t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical representation of the population\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Definitions of axis boundaries\n",
    "plt.xlim(X_projected[:,0].min(), X_projected[:,0].max())\n",
    "plt.ylim(X_projected[:,1].min(), X_projected[:,1].max())\n",
    "\n",
    "# Definition of axis'labels\n",
    "plt.title(\"t-SNE\\n\")\n",
    "plt.xlabel(\"t-SNE feature 1\")\n",
    "plt.ylabel(\"t-SNE feature 2\")\n",
    "\n",
    "# Mapping colors\n",
    "y = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "# Glyphes and colors are representing 'Nutri-Score grades'\n",
    "for i in range(len(X_projected)):\n",
    "    plt.scatter(X_projected[:,0][i], # x-coordinate\n",
    "             X_projected[:,1][i], # y-coordinate\n",
    "             # y.iloc[i,0], # labels\n",
    "             color=plt.cm.RdYlGn_r(y.iloc[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sauvegarde** (méthode *feather*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feather does not support serializing a non-default index\n",
    "data = data.reset_index()\n",
    "\n",
    "# Deleting index\n",
    "data = data.drop(columns=[\"index\"])\n",
    "\n",
    "# Save the file (binary)\n",
    "data.to_feather(\"p4_data4.ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
